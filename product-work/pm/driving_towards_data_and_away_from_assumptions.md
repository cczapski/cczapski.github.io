---
layout: product-work
case-study: Driving towards Data and Away from Assumptions
description: Initiating a more systematic, data-informed approach to product analysis and feature prioritization
permalink: /pm/driving-towards-data-and-away-from-assumptions
---

<h3 class="first-h3">The Problem</h3>
New to the task of product prioritization, during my first roadmap project I was intrigued to see how our team made feature decisions. The team consisted of the Sales Team's Account Executive, Marketing Product Manager and Director of Product roles, and the new addition of my role as the Technical Product Manager from the Software Development Team.

It became quite clear from the beginning just how decisions were made: various sales team members received feedback here and there from different customers, and thus the list to evaluate was generated from those, before the team debated their opinions on which of the suggestions should ultimately take priority in development. 

I was immediately confused and left with many outstanding questions:
* Are there more instances of other customers and/or prospects interested in the proposed features than the one or two that have expressed the need thus far? 
* What volume of user impact would one feature have over the other? 
* What type of return could we expect on one feature over the other?
* Are we weighing the paying customer's needs too heavily over other users critical needs? 
* What other problems have we not heard about from our non-vocal customers that could actually be more desired and needed for a larger user base than the ones proposed here? 
* How do we know that the proposed features are actually the best solution and approach to the root user problems? 
* And on, and on, and on . . .

After I asked a handful of these questions, it simply caused more debate of opinions and assumptions being tossed around and it was determined that there wasn't time further evaluations. The team would follow the same process they had used in the past, but were open to my suggestion on including more data evaluations for the next roadmap version.

From that point, my problem was identified: <span class="emphasis">what could I find and propose that would drive our product prioritization process towards a larger focus on data and away from assumptions, opinions, and guesswork for the next roadmap version?</span>

<h3 class="second-h3">The Details</h3>

<h5>The Application</h5>
All four <a href="/work/what-is-vineya" target="blank">Vineya</a> applications: Vineya for Agencies, Vineya for Businesses, Vineya for Interpreters, and Vineya Lighthouse (internal organization administrative platform)

<h5>The Users & Audience</h5>
The full product user base of external Agency and Business customers, ASL interpreters, Deaf and Hard of Hearing Consumers, and internal administrative employees

<h5>The Team & Role</h5>
I partnered with our Marketing Product Manager to evaluate, inform, and propose feature priorities for the upcoming Vineya Roadmap to the Director of Product and Software Development Manager, before ultimate presentation to our C-team. 

<h5>The Tools & Methods</h5>
Jira, SurveyMonkey, Product Data Reports, Excel, PowerPoint, Direct User Feedback and User Surveys, Data Analysis, Competitor Analysis, The RICE Method

<h3 class="third-h3">The Approach</h3>
Always my first step when starting new tasks, I researched software roadmap prioritization topics and immediately became overwhelmed with the volume of information on the topic. Process do's, process don'ts (all of which seemed to highlight not allowing the sales team to drive these decisionsâ€¦). 

I determined that I'd need something easily customizable to our product, that wasn't too evaluation heavy given our initial move away from zero data evaluations. In consideration of these goals, one method started to stand out much more than many of the others: RICE: Simple Prioritization for product managers. 

The focus of the RICE method is around four key factors: 
* Reach - estimate of the number of users a feature or idea would affect over a certain period of time
* Impact - estimate of the level of impact a feature or idea would have on one single user
* Confidence - estimate of the level of confidence you  have in the other scores based on the strength of the data informing those decisions
* Effort - estimate of the volume of effort needed to implement a feature or idea

Once the estimates for each factor is determined for each feature or idea, a simple equation leads you to the ultimate RICE score:

<p class="center-image">
    <img align="middle" class="img-shadow-dark" alt="Driving towards Data & Away from Assumptions" src="/images/work/pm/driving_towards_data/rice_equation_preview.png">
</p>
    
With some initial ideas on how we could determine Reach and Impact scores with product specific data points, I presented this approach to the team and we decided that we'd give it a go for our upcoming roadmap project. 

<h3 class="first-h3">The Solution</h3>
The roadmap evaluations were approached with a more data-centric analysis than previous roadmap evaluations. Given the various data sources for each feature or ability evaluated, the RICE scoring method was used to first generate, and then help rank and prioritize over 60+ items from the product backlog. 

Each of the initial items or features evaluated was included based on one or more of the below data points: 

* Volume of Customer Service feedback 
* Direct User feedback 
* User Needs Assessments / Survey results 
* Competitor Offering 
* Unproductive or Negative Internal Work 

We approached evaluations in three phases, with phases 1 and 2 focused around the RICE method.

##### Phase 1: Initial Feature Refinement
To consolidate a list of 60+ features down to 15, we evaluated the Impact score of each from the list of accessible data points and selected the highest priority items based on those results. 
    
<p>
    <img class="img-shadow-dark" alt="Impact Score" src="/images/work/pm/driving_towards_data/impact_score.png">
</p>
     
##### Phase 2: Additional Data Evaluations
The second refinement of the 15 feature set focused on evaluating all four categories of the RICE approach, with additional supporting data around development estimates (development time, complexity/Effort, resources and cost), and estimated revenue and cost. 

##### Phase 3: Final Feature Selection
The final feature selection and proposed development plan were determined by evaluating the full spectrum of data collected for each feature. Primary items of focus surrounded RICE scores, estimated revenue, estimated cost, and estimated time to develop

<p>
    <img class="img-shadow-dark" alt="Impact Score" src="/images/work/pm/driving_towards_data/full_rice_score.png">
</p>

<h3 class="second-h3">The Learnings</h3>
##### Some data is better than none
Despite the final decisions still being strongly influenced by the Sales Team's assumptions and opinions, the use of data and the RICE method drastically improved our evaluations and led us to a more data-centric roadmap than ever before. 

##### Organizational change is possible with valid, logical improvements to existing processes
At first a daunting task, guiding our process towards a more systematic approach actually was an achievable goal. Incremental process change is possible, even if you are not the ultimate decision maker in an organization, through valid and logical process improvements.

<p class="italic small-note">Have any thoughts, ideas, feedback? I love to discuss my work, so <a href="mailto:casiemattrisch@gmail.com">let me know</a>!</p>

<div class="resume">
  <button class="back">
      <a href="/pm/">Back to PM Work</a>
  </button>
</div>